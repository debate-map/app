apiVersion: v1
kind: Service
metadata:
  name: dm-app-server
  #namespace: app
  labels:
    app: dm-app-server
spec:
  #clusterIP: None
  selector:
    app: dm-app-server
  # to make it accessible outside of cluster
  #type: NodePort
  ports:
    - name: main
      port: 5110
      protocol: TCP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dm-app-server
  #namespace: app
  labels:
    app: dm-app-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dm-app-server
  template:
    metadata:
      labels:
        app: dm-app-server
    spec:
      imagePullSecrets:
        - name: registry-credentials
      containers:
      - name: dm-app-server
        image: "TILT_PLACEHOLDER:imageURL_appServer"
        resources:
          requests:
            memory: 500Mi
          limits:
            #cpu: "900m" # commented atm, due to limits apparently causing slowdown, even if limit not reached (see: https://erickhun.com/posts/kubernetes-faster-services-no-cpu-limits)
            # if it ends up using more than this, it is likely a memory leak; kill and restart should restore regular performance
            memory: 3000Mi # temp-increased to 3gb, to fix OOM issue hit (will try to debug/fix soon)
        livenessProbe:
          httpGet:
            path: /health-check
            port: 5110
          initialDelaySeconds: 10999999 # mode: profiling
          #initialDelaySeconds: 20 # mode: normal
          periodSeconds: 10
          timeoutSeconds: 3
        env:
        - name: TRUSTED_OPERATOR_PASSKEY
          valueFrom: { secretKeyRef: { name: debate-map-trusted-operator, key: passkey, optional: true } }
        - name: DB_VENDOR
          value: "postgres"
        - name: DB_ADDR
          valueFrom: { secretKeyRef: { name: debate-map-pguser-admin, key: host } }
        - name: DB_PORT
          valueFrom: { secretKeyRef: { name: debate-map-pguser-admin, key: port } }
        - name: DB_DATABASE
          valueFrom: { secretKeyRef: { name: debate-map-pguser-admin, key: dbname } }
        - name: DB_USER
          valueFrom: { secretKeyRef: { name: debate-map-pguser-admin, key: user } }
        - name: DB_PASSWORD
          valueFrom: { secretKeyRef: { name: debate-map-pguser-admin, key: password } }
        - name: PROXY_ADDRESS_FORWARDING
          value: "true"

# give app-server various kubernetes-api permissions, within its own namespace "default"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: role-for-dm-app-server
  namespace: default
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: [
    "get",
    "create",
  ]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: role-binding-for-dm-app-server
  namespace: default
subjects:
  - kind: ServiceAccount
    name: default
roleRef:
  kind: Role
  name: role-for-dm-app-server
  apiGroup: rbac.authorization.k8s.io

# give app-server various kubernetes-api permissions, *across namespaces*
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole # note that we use "ClusterRole" instead of just "Role", so granted permissions can work across namespaces
metadata:
  name: cluster-role-for-dm-app-server
  namespace: default
rules:
# probably temp; give app-server the ability to run commands in other pods
# (this is needed atm for the `getDBDump` endpoint, but is not ideal, as it "grants more power" than should be necessary for this need)
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods/exec"]
  verbs: ["create", "get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding # note: using ClusterRoleBinding (as matching above)
metadata:
  name: cluster-role-binding-for-dm-app-server
  namespace: default
subjects:
  - kind: ServiceAccount
    name: default
    namespace: default # this also becomes required
roleRef:
  kind: ClusterRole # note: using ClusterRole (as matching above)
  name: cluster-role-for-dm-app-server
  apiGroup: rbac.authorization.k8s.io