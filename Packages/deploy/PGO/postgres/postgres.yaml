apiVersion: postgres-operator.crunchydata.com/v1beta1
kind: PostgresCluster
metadata:
  name: debate-map
spec:
  image: registry.developers.crunchydata.com/crunchydata/crunchy-postgres-ha:centos8-13.3-1
  postgresVersion: 13
  #shutdown: true # uncomment this to shutdown all the postgres pods (other than the "pgo" controller, and the metrics-collecting pods like "crunchy-grafana")
  instances:
    - name: instance1
      dataVolumeClaimSpec:
        accessModes:
        - "ReadWriteOnce"
        resources:
          requests:
            storage: 10Gi
  # dataSource:
  #   postgresCluster:
  #     clusterName: debate-map
  #     repoName: repo2
  #     options:
  #     # use this to restore to a base-backup, without wal-archive replaying (modify "set" to the base-backup folder-name seen in the cloud-bucket)
  #     # NOTE: This approach doesn't currently work, unless you add a workaround. See here: https://github.com/CrunchyData/postgres-operator/issues/1886#issuecomment-907784977
  #     - --set 20211219-194118F_20211222-041751D
  #     - --type=immediate
  #     #- --target-action=promote
  #     # use this to restore to a specific point-in-time, with wal-archive replaying (modifying "target" to the time you want to restore to, with specified timezone [UTC recommended])
  #     # - --type=time
  #     # - --target="2021-09-01 07:42:06+00"
  backups:
    pgbackrest:
      image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:centos8-2.33-1
      repoHost:
        dedicated: {}
      configuration:
      - secret:
          name: pgo-gcs-creds
      global:
        repo2-path: /db-backups-pgbackrest
      repos:
      - name: repo1
        volume:
          volumeClaimSpec:
            accessModes:
            - "ReadWriteOnce"
            resources:
              requests:
                storage: 10Gi
      - name: repo2
        gcs:
          bucket: "TILT_PLACEHOLDER:bucket_uniformPrivate_name"
        schedules:
          # at 3am, each Sunday (cron schedule syntax)
          full: "0 3 * * 0"
          # at 3:10am, each day (cron schedule syntax)
          differential: "10 3 * * *"
      # configuration for the next manual backup to be triggered (see deploy/pg-backups guide-module for more info)
      manual:
        repoName: repo2
        options:
        - --type=full

      # configuration for the next restore to be triggered # commented; now set using the restoreDBBackup_prep script
      # restore:
      #   enabled: true
      #   repoName: repo2
      #   options:
      #   - --set 20210828-044420F
      #   - --type=time
      #   - --target="2021-08-28 04:44:20 UTC"

      # restore: {
      #   enabled: true,
      #   repoName: "repo2",
      #   options: [
      #     "--delta",
      #     "--force",
      #     "--set 20210828-074206F"
      #   ]
      # }
  patroni:
    dynamicConfiguration:
      postgresql:
        #parameters:
        #  max_parallel_workers: 2
        #  max_worker_processes: 2
        #  shared_buffers: 1GB
        #  work_mem: 2MB

        # values for parameters based on [https://pgtune.leopard.in.ua] and [https://stackoverflow.com/a/32584211]
        # NOTE: To apply these changes:
        # 1) Make sure kubectl/docker-desktop has the right cluster set. (use docker-desktop's tray-icon)
        # 2) Run this in repo root: `kubectl apply -k Packages/deploy/PGO/postgres`
        # 3) This "shouldn't" be necessary, but atm I've found that I need to kill the "debate-map-instance1-XXX" pod for changes to be applied (at least quickly); after doing so, the pod will restart in a half minute or so.
        # 4) Confirm that changes were applied, by using Lens to open terminal in the "debate-map-instance1-XXX" pod, running `psql`, then running: `SHOW max_connections;`
        parameters:
          # for 15gb node
          max_connections: 300 # default: 100

          # for 30gb node
          # max_connections: 1000 # default: 100
          # shared_buffers: 5GB # default: 8MB/128MB
          # maintenance_work_mem: 1GB # default: 64MB
          # effective_cache_size: 10GB # default: 4GB
          # #work_mem: 10MB # default: 4MB # not increasing this atm, as it's "per connection", and we have so many
        pg_hba:
          - host all all 0.0.0.0/0 md5
  monitoring:
    pgmonitor:
      exporter:
        image: registry.developers.crunchydata.com/crunchydata/crunchy-postgres-exporter:ubi8-5.0.2-0
  users:
    - name: admin
      databases:
        - debate-map
      options: "SUPERUSER"
  metadata:
    annotations:
      # make-so the user-secret can be mirrored to the "app" namespace (see user-secret-mirror.yaml for the annotation on the "pulling" end)
      reflector.v1.k8s.emberstack.com/reflection-allowed: "true"
      #reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: "app"
      reflector.v1.k8s.emberstack.com/reflection-allowed-namespaces: "default"
      #reflector.v1.k8s.emberstack.com/reflection-auto-enabled: "true"
      #reflector.v1.k8s.emberstack.com/reflection-auto-namespaces: "app"