apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-setup
  namespace: kube-system
  labels:
    k8s-app: node-setup
spec:
  selector:
    matchLabels:
      name: node-setup
  template:
    metadata:
      labels:
        name: node-setup
    spec:
      #nodeSelector:
      #  disktype: nvme
      containers:
      - name: node-setup
        image: ubuntu
        command: ["/bin/sh","-c"]
        args: ["/script/node-setup.sh"]
        env:
          - name: PARTITION_NUMBER
            valueFrom:
              configMapKeyRef:
                name: node-setup-config
                key: partition_number
        volumeMounts:
          - name: node-setup-script
            mountPath: /script
          - name: dev
            mountPath: /dev
          - name: etc-lvm
            mountPath: /etc/lvm
        securityContext:
          allowPrivilegeEscalation: true
          privileged: true
      volumes:
        - name: node-setup-script
          configMap:
            name: node-setup-script
            defaultMode: 0755
        - name: dev
          hostPath:
            path: /dev
        - name: etc-lvm
          hostPath:
            path: /etc/lvm
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-setup-config
  namespace: kube-system
data:
  partition_number: "3"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-setup-script
  namespace: kube-system
data:
  node-setup.sh: |
    #!/bin/bash
    set -e

    # change the file-watcher max-count on each node to 524288 (needed for "watch:**" to work in the ecosystem.config.cjs files)
    #echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p
    sysctl -w fs.inotify.max_user_watches=524288

    # also set this (fixes error: "failed to create fsnotify watcher: too many open files")
    sysctl -w fs.inotify.max_user_instances=100000

    # this increases the max # of mmap()'ed ranges (as per https://stackoverflow.com/a/59923848), in an attempt to fix...
    # ... the dreadful issue where trying to create a memory-snapshot using remote Chrome devtools would virtually always cause the pod to crash (or at least for the port-forward to dc)
    sysctl -w vm.max_map_count=65530999

    # check that the new values were applied
    echo "New value for max_user_watches:"
    cat /proc/sys/fs/inotify/max_user_watches
    echo "New value for max_user_instances:"
    cat /proc/sys/fs/inotify/max_user_instances
    echo "New value for max_map_count:"
    cat /proc/sys/vm/max_map_count

    echo "Now sleeping forever... (so k8s doesn't think this pod died, and thus restart it)"
    sleep infinity